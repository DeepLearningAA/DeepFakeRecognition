{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import torchvision\n",
    "from torchvision import models\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from torch.optim import lr_scheduler\n",
    "\n",
    "from torch.utils.data import random_split, DataLoader\n",
    "from dataset_handlers.vgg16.vgg16_feature_dataset import FeatureDataset\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "import os\n",
    "from PIL import Image\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "vit_b = models.vit_b_16(pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "vit_b.heads = nn.Sequential(\n",
    "    nn.Linear(768, 2)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in vit_b.conv_proj.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "for param in vit_b.encoder.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in vit_b.heads.parameters():\n",
    "    param.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(model, data_loader, transform):\n",
    "    acc = 0\n",
    "    for i, (image, label) in enumerate(data_loader, 1):\n",
    "        image = image.to(device)\n",
    "        label = label.to(device)\n",
    "\n",
    "        image = transform(image)\n",
    "\n",
    "        output = model(image).reshape(-1, 2)\n",
    "        acc += (torch.argmax(output, dim=1) == label).sum().item()\n",
    "\n",
    "    return acc / len(data_loader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(num_epochs, model, criterion, optimizer, acc_training_set, acc_val_set, l1_factor, train_loader, val_loader):\n",
    "    for epoch in range(num_epochs):\n",
    "        for i, (image, label) in enumerate(train_loader, 1):\n",
    "            image = image.to(device)\n",
    "            label = label.to(device)\n",
    "\n",
    "            output = model(image)\n",
    "\n",
    "            loss = criterion(output, label)\n",
    "\n",
    "            l1_regularization = torch.tensor(0., requires_grad=False)\n",
    "            for param in model.parameters():\n",
    "                l1_regularization += torch.norm(param, 1)\n",
    "\n",
    "            l1_regularization.requires_grad_(True)\n",
    "            loss += l1_factor * l1_regularization\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            if i % 1 == 0:\n",
    "                print('Epoch: {:2.0f}/{}, Batch: {:3.0f}, Loss: {:.6f}'\n",
    "                      .format(epoch+1, num_epochs, i, loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 5\n",
    "l1_factor = 0.0001\n",
    "l2_factor = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "critereon = nn.CrossEntropyLoss()\n",
    "optimizer = optim.AdamW(vit_b.parameters(), lr=0.0001, weight_decay=l2_factor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor()\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = torchvision.datasets.ImageFolder(\n",
    "    root='data/splitted/train',\n",
    "    transform=transform\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  1/5, Batch:   1, Loss: 182.879929\n",
      "Epoch:  1/5, Batch:   2, Loss: 182.923660\n",
      "Epoch:  1/5, Batch:   3, Loss: 183.078873\n",
      "Epoch:  1/5, Batch:   4, Loss: 182.918396\n",
      "Epoch:  1/5, Batch:   5, Loss: 182.949097\n",
      "Epoch:  1/5, Batch:   6, Loss: 182.955841\n",
      "Epoch:  1/5, Batch:   7, Loss: 182.913269\n",
      "Epoch:  1/5, Batch:   8, Loss: 182.919449\n",
      "Epoch:  1/5, Batch:   9, Loss: 182.944977\n",
      "Epoch:  1/5, Batch:  10, Loss: 182.926666\n",
      "Epoch:  1/5, Batch:  11, Loss: 182.929642\n",
      "Epoch:  1/5, Batch:  12, Loss: 182.932648\n",
      "Epoch:  1/5, Batch:  13, Loss: 182.921082\n",
      "Epoch:  1/5, Batch:  14, Loss: 182.902588\n",
      "Epoch:  1/5, Batch:  15, Loss: 182.933762\n",
      "Epoch:  1/5, Batch:  16, Loss: 182.917984\n",
      "Epoch:  1/5, Batch:  17, Loss: 182.903412\n",
      "Epoch:  1/5, Batch:  18, Loss: 182.932938\n",
      "Epoch:  1/5, Batch:  19, Loss: 182.937546\n",
      "Epoch:  1/5, Batch:  20, Loss: 182.952896\n",
      "Epoch:  1/5, Batch:  21, Loss: 182.963013\n",
      "Epoch:  1/5, Batch:  22, Loss: 182.984543\n",
      "Epoch:  1/5, Batch:  23, Loss: 182.883240\n",
      "Epoch:  1/5, Batch:  24, Loss: 182.919525\n",
      "Epoch:  1/5, Batch:  25, Loss: 182.964905\n",
      "Epoch:  1/5, Batch:  26, Loss: 182.909988\n",
      "Epoch:  1/5, Batch:  27, Loss: 182.929550\n",
      "Epoch:  1/5, Batch:  28, Loss: 182.932755\n",
      "Epoch:  1/5, Batch:  29, Loss: 182.888947\n",
      "Epoch:  1/5, Batch:  30, Loss: 182.954956\n",
      "Epoch:  1/5, Batch:  31, Loss: 182.945145\n",
      "Epoch:  1/5, Batch:  32, Loss: 182.910416\n",
      "Epoch:  1/5, Batch:  33, Loss: 182.942719\n",
      "Epoch:  1/5, Batch:  34, Loss: 182.956802\n",
      "Epoch:  1/5, Batch:  35, Loss: 182.911804\n",
      "Epoch:  1/5, Batch:  36, Loss: 182.963837\n",
      "Epoch:  1/5, Batch:  37, Loss: 182.900192\n",
      "Epoch:  1/5, Batch:  38, Loss: 182.985687\n",
      "Epoch:  1/5, Batch:  39, Loss: 182.916122\n",
      "Epoch:  1/5, Batch:  40, Loss: 182.949463\n",
      "Epoch:  1/5, Batch:  41, Loss: 182.922714\n",
      "Epoch:  2/5, Batch:   1, Loss: 182.938721\n",
      "Epoch:  2/5, Batch:   2, Loss: 182.958710\n",
      "Epoch:  2/5, Batch:   3, Loss: 182.907654\n",
      "Epoch:  2/5, Batch:   4, Loss: 182.911743\n",
      "Epoch:  2/5, Batch:   5, Loss: 182.911224\n",
      "Epoch:  2/5, Batch:   6, Loss: 182.922180\n",
      "Epoch:  2/5, Batch:   7, Loss: 182.905609\n",
      "Epoch:  2/5, Batch:   8, Loss: 182.940811\n",
      "Epoch:  2/5, Batch:   9, Loss: 182.946030\n",
      "Epoch:  2/5, Batch:  10, Loss: 182.909210\n",
      "Epoch:  2/5, Batch:  11, Loss: 182.960220\n",
      "Epoch:  2/5, Batch:  12, Loss: 182.857727\n",
      "Epoch:  2/5, Batch:  13, Loss: 182.883896\n",
      "Epoch:  2/5, Batch:  14, Loss: 182.953934\n",
      "Epoch:  2/5, Batch:  15, Loss: 182.914566\n",
      "Epoch:  2/5, Batch:  16, Loss: 182.907776\n",
      "Epoch:  2/5, Batch:  17, Loss: 182.933044\n",
      "Epoch:  2/5, Batch:  18, Loss: 182.934174\n",
      "Epoch:  2/5, Batch:  19, Loss: 182.937500\n",
      "Epoch:  2/5, Batch:  20, Loss: 182.908005\n",
      "Epoch:  2/5, Batch:  21, Loss: 182.913849\n",
      "Epoch:  2/5, Batch:  22, Loss: 182.918655\n",
      "Epoch:  2/5, Batch:  23, Loss: 182.907776\n",
      "Epoch:  2/5, Batch:  24, Loss: 182.932953\n",
      "Epoch:  2/5, Batch:  25, Loss: 182.935837\n",
      "Epoch:  2/5, Batch:  26, Loss: 182.928284\n",
      "Epoch:  2/5, Batch:  27, Loss: 182.892197\n",
      "Epoch:  2/5, Batch:  28, Loss: 182.922882\n",
      "Epoch:  2/5, Batch:  29, Loss: 182.949509\n",
      "Epoch:  2/5, Batch:  30, Loss: 182.934937\n",
      "Epoch:  2/5, Batch:  31, Loss: 182.945923\n",
      "Epoch:  2/5, Batch:  32, Loss: 182.913361\n",
      "Epoch:  2/5, Batch:  33, Loss: 182.907104\n",
      "Epoch:  2/5, Batch:  34, Loss: 182.910019\n",
      "Epoch:  2/5, Batch:  35, Loss: 182.930679\n",
      "Epoch:  2/5, Batch:  36, Loss: 182.926727\n",
      "Epoch:  2/5, Batch:  37, Loss: 182.903259\n",
      "Epoch:  2/5, Batch:  38, Loss: 182.926834\n",
      "Epoch:  2/5, Batch:  39, Loss: 182.892395\n",
      "Epoch:  2/5, Batch:  40, Loss: 182.859177\n",
      "Epoch:  2/5, Batch:  41, Loss: 182.900711\n",
      "Epoch:  3/5, Batch:   1, Loss: 182.874786\n",
      "Epoch:  3/5, Batch:   2, Loss: 182.926025\n",
      "Epoch:  3/5, Batch:   3, Loss: 182.894547\n",
      "Epoch:  3/5, Batch:   4, Loss: 182.951340\n",
      "Epoch:  3/5, Batch:   5, Loss: 182.922058\n",
      "Epoch:  3/5, Batch:   6, Loss: 182.875519\n",
      "Epoch:  3/5, Batch:   7, Loss: 182.872391\n",
      "Epoch:  3/5, Batch:   8, Loss: 182.887878\n",
      "Epoch:  3/5, Batch:   9, Loss: 182.922440\n",
      "Epoch:  3/5, Batch:  10, Loss: 182.893143\n",
      "Epoch:  3/5, Batch:  11, Loss: 182.902710\n",
      "Epoch:  3/5, Batch:  12, Loss: 182.892685\n",
      "Epoch:  3/5, Batch:  13, Loss: 182.880112\n",
      "Epoch:  3/5, Batch:  14, Loss: 182.893112\n",
      "Epoch:  3/5, Batch:  15, Loss: 182.879730\n",
      "Epoch:  3/5, Batch:  16, Loss: 182.901535\n",
      "Epoch:  3/5, Batch:  17, Loss: 182.885132\n",
      "Epoch:  3/5, Batch:  18, Loss: 182.898239\n",
      "Epoch:  3/5, Batch:  19, Loss: 182.892059\n",
      "Epoch:  3/5, Batch:  20, Loss: 182.938507\n",
      "Epoch:  3/5, Batch:  21, Loss: 182.879837\n",
      "Epoch:  3/5, Batch:  22, Loss: 182.872101\n",
      "Epoch:  3/5, Batch:  23, Loss: 182.883881\n",
      "Epoch:  3/5, Batch:  24, Loss: 182.892197\n",
      "Epoch:  3/5, Batch:  25, Loss: 182.901230\n",
      "Epoch:  3/5, Batch:  26, Loss: 182.890198\n",
      "Epoch:  3/5, Batch:  27, Loss: 182.895706\n",
      "Epoch:  3/5, Batch:  28, Loss: 182.880173\n",
      "Epoch:  3/5, Batch:  29, Loss: 182.879364\n",
      "Epoch:  3/5, Batch:  30, Loss: 182.893677\n",
      "Epoch:  3/5, Batch:  31, Loss: 182.877625\n",
      "Epoch:  3/5, Batch:  32, Loss: 182.892212\n",
      "Epoch:  3/5, Batch:  33, Loss: 182.897354\n",
      "Epoch:  3/5, Batch:  34, Loss: 182.912277\n",
      "Epoch:  3/5, Batch:  35, Loss: 182.886017\n",
      "Epoch:  3/5, Batch:  36, Loss: 182.876083\n",
      "Epoch:  3/5, Batch:  37, Loss: 182.861694\n",
      "Epoch:  3/5, Batch:  38, Loss: 182.870514\n",
      "Epoch:  3/5, Batch:  39, Loss: 182.883392\n",
      "Epoch:  3/5, Batch:  40, Loss: 182.938324\n",
      "Epoch:  3/5, Batch:  41, Loss: 182.877502\n",
      "Epoch:  4/5, Batch:   1, Loss: 182.872665\n",
      "Epoch:  4/5, Batch:   2, Loss: 182.876968\n",
      "Epoch:  4/5, Batch:   3, Loss: 182.851395\n",
      "Epoch:  4/5, Batch:   4, Loss: 182.880539\n",
      "Epoch:  4/5, Batch:   5, Loss: 182.901047\n",
      "Epoch:  4/5, Batch:   6, Loss: 182.885986\n",
      "Epoch:  4/5, Batch:   7, Loss: 182.873413\n",
      "Epoch:  4/5, Batch:   8, Loss: 182.867661\n",
      "Epoch:  4/5, Batch:   9, Loss: 182.907318\n",
      "Epoch:  4/5, Batch:  10, Loss: 182.912659\n",
      "Epoch:  4/5, Batch:  11, Loss: 182.851089\n",
      "Epoch:  4/5, Batch:  12, Loss: 182.817032\n",
      "Epoch:  4/5, Batch:  13, Loss: 182.846359\n",
      "Epoch:  4/5, Batch:  14, Loss: 182.840729\n",
      "Epoch:  4/5, Batch:  15, Loss: 182.894028\n",
      "Epoch:  4/5, Batch:  16, Loss: 182.852631\n",
      "Epoch:  4/5, Batch:  17, Loss: 182.862427\n",
      "Epoch:  4/5, Batch:  18, Loss: 182.875839\n",
      "Epoch:  4/5, Batch:  19, Loss: 182.865341\n",
      "Epoch:  4/5, Batch:  20, Loss: 182.847534\n",
      "Epoch:  4/5, Batch:  21, Loss: 182.877487\n",
      "Epoch:  4/5, Batch:  22, Loss: 182.846039\n",
      "Epoch:  4/5, Batch:  23, Loss: 182.885498\n",
      "Epoch:  4/5, Batch:  24, Loss: 182.916061\n",
      "Epoch:  4/5, Batch:  25, Loss: 182.861740\n",
      "Epoch:  4/5, Batch:  26, Loss: 182.831558\n",
      "Epoch:  4/5, Batch:  27, Loss: 182.857529\n",
      "Epoch:  4/5, Batch:  28, Loss: 182.878021\n",
      "Epoch:  4/5, Batch:  29, Loss: 182.888885\n",
      "Epoch:  4/5, Batch:  30, Loss: 182.842682\n",
      "Epoch:  4/5, Batch:  31, Loss: 182.910767\n",
      "Epoch:  4/5, Batch:  32, Loss: 182.894638\n",
      "Epoch:  4/5, Batch:  33, Loss: 182.872803\n",
      "Epoch:  4/5, Batch:  34, Loss: 182.886932\n",
      "Epoch:  4/5, Batch:  35, Loss: 182.887558\n",
      "Epoch:  4/5, Batch:  36, Loss: 182.873032\n",
      "Epoch:  4/5, Batch:  37, Loss: 182.892456\n",
      "Epoch:  4/5, Batch:  38, Loss: 182.879700\n",
      "Epoch:  4/5, Batch:  39, Loss: 182.894684\n",
      "Epoch:  4/5, Batch:  40, Loss: 182.861191\n",
      "Epoch:  4/5, Batch:  41, Loss: 182.851761\n",
      "Epoch:  5/5, Batch:   1, Loss: 182.823257\n",
      "Epoch:  5/5, Batch:   2, Loss: 182.883606\n",
      "Epoch:  5/5, Batch:   3, Loss: 182.821594\n",
      "Epoch:  5/5, Batch:   4, Loss: 182.836136\n",
      "Epoch:  5/5, Batch:   5, Loss: 182.858337\n",
      "Epoch:  5/5, Batch:   6, Loss: 182.826279\n",
      "Epoch:  5/5, Batch:   7, Loss: 182.846191\n",
      "Epoch:  5/5, Batch:   8, Loss: 182.834930\n",
      "Epoch:  5/5, Batch:   9, Loss: 182.824203\n",
      "Epoch:  5/5, Batch:  10, Loss: 182.850418\n",
      "Epoch:  5/5, Batch:  11, Loss: 182.896393\n",
      "Epoch:  5/5, Batch:  12, Loss: 182.821381\n",
      "Epoch:  5/5, Batch:  13, Loss: 182.842422\n",
      "Epoch:  5/5, Batch:  14, Loss: 182.900681\n",
      "Epoch:  5/5, Batch:  15, Loss: 182.856247\n",
      "Epoch:  5/5, Batch:  16, Loss: 182.834564\n",
      "Epoch:  5/5, Batch:  17, Loss: 182.865936\n",
      "Epoch:  5/5, Batch:  18, Loss: 182.823822\n",
      "Epoch:  5/5, Batch:  19, Loss: 182.827850\n",
      "Epoch:  5/5, Batch:  20, Loss: 182.852783\n",
      "Epoch:  5/5, Batch:  21, Loss: 182.808685\n",
      "Epoch:  5/5, Batch:  22, Loss: 182.862885\n",
      "Epoch:  5/5, Batch:  23, Loss: 182.848969\n",
      "Epoch:  5/5, Batch:  24, Loss: 182.851212\n",
      "Epoch:  5/5, Batch:  25, Loss: 182.873535\n",
      "Epoch:  5/5, Batch:  26, Loss: 182.859375\n",
      "Epoch:  5/5, Batch:  27, Loss: 182.881378\n",
      "Epoch:  5/5, Batch:  28, Loss: 182.807938\n",
      "Epoch:  5/5, Batch:  29, Loss: 182.900772\n",
      "Epoch:  5/5, Batch:  30, Loss: 182.822769\n",
      "Epoch:  5/5, Batch:  31, Loss: 182.837265\n",
      "Epoch:  5/5, Batch:  32, Loss: 182.843414\n",
      "Epoch:  5/5, Batch:  33, Loss: 182.860657\n",
      "Epoch:  5/5, Batch:  34, Loss: 182.839264\n",
      "Epoch:  5/5, Batch:  35, Loss: 182.845352\n",
      "Epoch:  5/5, Batch:  36, Loss: 182.830566\n",
      "Epoch:  5/5, Batch:  37, Loss: 182.868469\n",
      "Epoch:  5/5, Batch:  38, Loss: 182.861465\n",
      "Epoch:  5/5, Batch:  39, Loss: 182.811874\n",
      "Epoch:  5/5, Batch:  40, Loss: 182.843430\n",
      "Epoch:  5/5, Batch:  41, Loss: 182.811417\n"
     ]
    }
   ],
   "source": [
    "acc_training_set = []\n",
    "acc_val_set = []\n",
    "\n",
    "for fold in range(1):\n",
    "    train_size = int(0.8 * len(dataset))\n",
    "    val_size = len(dataset) - train_size\n",
    "    train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "    train_loader = DataLoader(dataset=train_dataset, batch_size=32, shuffle=True)\n",
    "    val_loader = DataLoader(dataset=val_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "    train_model(epochs, vit_b, critereon, optimizer, acc_training_set, acc_val_set, l1_factor, train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = torchvision.datasets.ImageFolder(\n",
    "    root='data/splitted/test',\n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=32, shuffle=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aa1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
